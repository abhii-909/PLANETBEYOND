# -*- coding: utf-8 -*-
"""nasamodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dp8WUvVwtUAdtjSCRHBZkG-4gNzKNyRg
"""

import pandas as pd
import numpy as np
import random

# Define the number of users and interactions
num_users = 30
num_interactions = 300  # each user has multiple interactions with quizzes

# Define the number of categories and quizzes
num_categories = 3
num_quizzes_per_category = 10

# Define the categories
categories = {
    1: 'Exoplanet Discovery Methods',
    2: 'Habitability and Life on Exoplanets',
    3: 'Exoplanet Types and Characteristics'
}

# Randomly assign learning styles
learning_styles = ['visual', 'auditory', 'kinesthetic']

# Generate random user-category-quiz interactions
random_data = {
    'user_id': np.random.randint(1, 101, size=num_interactions),  # User IDs from 1 to 100
    'category_id': np.random.randint(1, num_categories + 1, size=num_interactions),
    'quiz_id': [random.randint(1, num_quizzes_per_category) for _ in range(num_interactions)],  # Quiz IDs range from 1 to 10 for each category
    'rating': np.random.randint(1, 6, size=num_interactions),  # Ratings between 1 and 5
    'learning_style': [random.choice(learning_styles) for _ in range(num_interactions)]  # Random learning styles
}

# Create a DataFrame
df_random_data = pd.DataFrame(random_data)

# Map the category_id to category names
df_random_data['category_name'] = df_random_data['category_id'].map(categories)

# Show the first few rows of the generated data
print(df_random_data.head())

# Save to a CSV file if needed
df_random_data.to_csv('exoplanet_learning_data.csv', index=False)

!pip install scikit-surprise
import pandas as pd
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise import accuracy
from surprise import KNNBasic
from surprise.model_selection import GridSearchCV
import numpy as np

# Load the generated dataset
df = pd.read_csv('exoplanet_learning_data.csv')

# Preview the data
print("Data Preview:")
print(df.head())

# Define the Reader for the Surprise library
reader = Reader(rating_scale=(1, 5))

# Create the Surprise dataset
data = Dataset.load_from_df(df[['user_id', 'quiz_id', 'rating']], reader)

# Split the dataset into training and testing sets
trainset, testset = train_test_split(data, test_size=0.2)

# Initialize the SVD (Singular Value Decomposition) model
model = SVD()

# Train the model on the training set
model.fit(trainset)

# Test the model's accuracy on the test set
predictions = model.test(testset)
rmse = accuracy.rmse(predictions)

print(f"RMSE of the SVD model: {rmse}")

# Recommend top N quizzes for a specific user
def get_top_n(predictions, n=5):
    top_n = {}
    for uid, iid, true_r, est, _ in predictions:
        if uid not in top_n:
            top_n[uid] = []
        # Calculate percentage
        percentage = ((est - 1) / 4) * 100
        top_n[uid].append((iid, est, percentage))

    # Sort the predictions for each user and retrieve the highest-rated quizzes
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

# Now when you get recommendations, it will include the percentage
top_n_recommendations = get_top_n(predictions, n=5)

# Display the recommendations with percentages
for user, recommendations in top_n_recommendations.items():
    print(f"Top quiz recommendations for user {user}:")
    for quiz_id, est_rating, percentage in recommendations:
        print(f" - Quiz ID: {quiz_id}, Predicted Rating: {est_rating:.2f}, Percentage: {percentage:.2f}%")